# Team Report:


## Previous Week’s Goals



* Beta release, due 2/13
* Frontend:
    * Add next elements to the UI explained above (Sulaiman)
        * 2 days, due 2/9
    * Use endpoints w/ some sort of classification can be a dummy (Sulaiman)
        * 3 days, due 2/10
* Backend:
    * Set up API structure  (Sulaiman)
        * 1 day, due 2/8
    * Attach CV model and LLM to server
        * 2 days, due 2/10
    * Set up hosting and CD (Aaron)
        * 2 days, due 2/12
    * Create tests with images (Aaron)
        * 1 day, due 2/10
* CV:
    * Continue training and improving CV models (Victor)
        * 3 days, due 2/11
* ML:
    * Finalize LLM access, through api or local hosting (Aneesha)
        * 3 days, due 2/11
    * More testing and prompt engineering


## Meeting Agenda



* Scrum stand-up (5 min)
* Questions (10-15 min)


## Progress and Issues

This week we finished our beta release of the app! We have a working version that can take an image, send it to a locally running backend server which processes the image and responds with the waste type of the item.

Our two main issues are CV model accuracy and LLM speed. We can likely significantly improve the object detection accuracy by switching models to a pre-trained version, but at the moment a possible blocker is speeding up the LLM since we don’t have much control over how that runs.

A third issue is setting up hosting, since our LLM takes much more memory than the free tier of AWS EC2 allows. We do have possible alternatives and more research to do, though.


## Plans and Goals



* Frontend (due by 2/17)
    * Work on classify text to work like images
    * Ensure it works across all android devices
    * Work on uploading images instead of taking an image
* Backend:
    * Set up hosting and CD (Aaron)
        * 2 days, due 2/19
    * Create tests with images (Aaron)
        * 1 day, due 2/16
* CV:
    * Get a working transfer learning model (Victor)
        * 3 days
    * Integrate model with backend (Victor)
        * 2 days
* ML:
    * LLM should send location-aware responses to front end (Aneesha)
        * 3 days, due Tuesday
    * Look into how to improve speed of LLM (Aneesha)
        * 2 days, Friday


# Individual Reports:


## Sulaiman:


### Previous Week’s Goals



* Frontend (due Thursday/Friday):
    * Add transition to show photo in another interface w/ loading screen 
    * Add back button, reverse button, ensuring alignment
    * Save image so it can be fed into model
    * Use API endpoints w/ POST requests
* Backend (due Thursday/Friday)
    * Define and create API endpoints routes for classify image, classify text, and report classification and document the JSON structure
    * Test these endpoints


### Progress and Issues

This week I was able to finish the transition to another interface after taking a picture with a loading logo until the output is given to show to the user where to throw away their object. I added the back button as well, but haven’t got to the reverse button but that’s easy all I need is a physical device or have someone else test the reverse on their end if double tap does not work. Next I was able to complete the POST request to send the image to the cv model and saving it as a jpeg file and waiting for a classification response. Next steps for me will be to add more tests and work on the next steps which include UI enhancements and the stretch goals and making sure the app works across devices.	 


### Plans and Goals



* Frontend (due by 2/17)
    * Work on classify text to work like images
    * Ensure it works across all android devices
    * Work on uploading images instead of taking an image
* Backend (due by 2/17)
    * Work on text classify text API request


## Aneesha:


### Previous Week’s Goals



* Finalize LLM access, through api or local hosting (3 days, due 2/11)


### Progress and Issues

This week I was able to figure out how to run the LLM locally and set it up so that it receives the object classification from the CV model and sends how it should be disposed of to the frontend. The issue currently is that it runs really slow, so this week I will test other LLMs to see if they are more efficient or see how to fix that issue. I also helped with issues that arose during the integration of the entire pipeline.


### Plans and Goals



* LLM should send location-aware responses to front end (3 days, due Tuesday)
* Look into how to improve speed of LLM (2 days, Friday)


## Aaron:


### Previous Week’s Goals



* Set up hosting and CD for backend
    * Create AWS EC2 instance for staging
    * Set up CD to deploy on git tag
* Create tests for backend with images
    * Feeds image into algorithm and expects the correct waste type


### Progress and Issues

This week I worked to support our beta release. After other group members got the pieces in place, I helped test, diagnose, and fix issues with the integration of our frontend and backend, eventually resulting in a working (albeit slow) beta version of the app.

I also did some more preparation for hosting the app on AWS, but ultimately I chose to prioritize the integration over hosting, ensuring that we have a locally working version before deploying to a staging server.


### Plans and Goals



* Set up hosting and CD for backend
    * Create AWS EC2 instance for staging
    * Set up CD to deploy on git tag
* Create tests for backend with images
    * Feeds image into algorithm and expects the correct waste type
* Support other team members in improving model accuracy and speed


## Victor:


### Previous Week’s Goals



* Get a working transfer learning model 
    * 3 days
* Integrate model with backend
    * 2 days


### Progress and Issues

After the TA meeting on Thursday, I pivoted from continuing to improve the accuracy of our CV model by building a transfer learning model into getting our already functioning model into a more workable state. This involved both building the code around the model used to feed it images and handle responses, but also test cases and CI integration. The latter proved somewhat challenging as the model is hosted online accessed by an API key, and hiding that key while using GitHub Actions was tricky. Afterwards I worked on integration and we were successfully able to build a beta. 


### Plans and Goals



* Clean up the model code
    * 3 days
    * The current code on the main branch is pretty messy, mainly just scraped together to make the beta due date. It could be a lot better which would not only help in future testing, but can also be made more modular so switching out image classifiers is easier.
* Add more tests to the model
    * 2 days
    * Following from the previous goal, the new code would be easier to write tests for so I’m planning on having better test coverage for it.
* Begin documentation for CV code
    * 1 day


## Julia:


### Previous Week’s Goals



* For the upcoming week, my goals align with Sulaiman's and the team's overall objectives. Specifically, I aim to continue refining the UI design, incorporating any necessary tweaks for improved user experience and seamless integration with backend functionalities. 
* I will focus on implementing the next elements of the UI, such as transitions to show captured photos in a separate interface with a loading screen, adding navigation buttons, and ensuring alignment across different device orientations. 
* These tasks are scheduled to be completed by Thursday/Friday, in line with the team's timeline for the beta release.


### Progress and Issues

This week, I focused on refining the UI design, working closely with Sulaiman to ensure a seamless user experience. I successfully implemented transitions to display captured photos in a separate interface, complete with a loading screen to inform users that processing is underway. I added navigation buttons, including a back button, to enhance app navigability. I encountered challenges in ensuring UI alignment across different Android devices, a critical aspect given the variety of screen sizes and resolutions. However, I made significant progress by testing on multiple devices and adjusting the UI elements accordingly.


### Plans and Goals



* Collaborate on Classify Text UI Integration (due by 2/17): In line with the team's goal to work on classifying text to work like images, I will collaborate with Sulaiman to integrate this functionality into the UI. This involves designing and implementing a user-friendly interface for text input and displaying classification results.
* Further UI Enhancements (due by 2/17): I plan to continue refining the UI, focusing on responsiveness and alignment across all Android devices. This includes optimizing the layout for different screen sizes and resolutions to ensure a consistent user experience.
